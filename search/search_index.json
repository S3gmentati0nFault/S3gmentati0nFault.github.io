{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome on my portfolio website!</p> <p>I'm a computer science student at the end of my master studies at University of Milan, I have been interested in past years in the field of distributed systems technologies and cloud environments, I am now moving towards the field of applied machine learning after the course of Machine Learning for Physics held by professor Tilman Plehn at Karl Ruprecht university of Heidelberg.</p> <p>I am in the process of starting my master Thesis titled \"Supervised machine learning techniques for quench detecction in superconductors\" accompanied by both professors Dario Malchiodi and Lucio Rossi.</p>"},{"location":"algoweb/readme/","title":"Algoweb","text":"<p>The project can be found on Github (Algoweb project)</p> Introduction <p>Review of the lecture notes for the course of Web algorithmics, held by professor Sebastiano Vigna at Universit\u00e0 degli studi di Milano, the lecture notes are in italian and are updated to the end of last year.</p> <p>They are still a very viable option for all the students that take the course.</p> <p>I consider this project alongside my other project for the course of Discrete Structures 1 to be a good showcase of my abilities with the LaTeX language. To see the latest revision of the notes just follow the link below.</p> <p>Download Algoweb 2.0</p>"},{"location":"autoencoders/readme/","title":"Anomaly detection","text":"<p>The project can be found on Github (Autoencoders project), consider that the code cannot be executed unless a different dataset is provided (the original dataset url was taken down).</p> Introduction <p>I did this project alongside Mario Massimo (his github) during my stay in Heidelberg for the course of Machine Learning and Physics held by professor Tilman Plehn, the project was aimed at comparing the performance of two different machine learning architectures, the first one being an MLP based autoencoder and the second one being a CNN based autoencoder.</p> <p>The dataset came from the professor and a given preprocessing step was applied to turn the tracker information coming from the LHC into 40x40 images.</p> <p>The task at hand was: given the dataset, containing two different types of jets (Quantum Chromo Dynamics and Top), use one of the two jets as signal and the other as anomaly. The following image is an averaging of over 5000 jets taken from FPGA-Accelerated Machine Learning Inference as a Service for Particle Physics Computing and it is clear to see that the area spanned by the top jets is way wider than the one spanned by the QCD jets. That is because of the three-pronged structure of the top jets, which is not present in the QCD jets. </p> Top (right) vs QCD (left) jets <p>When it comes to a more practical scenario, whenever top jets are signal and QCD jets are anomaly it's quite likely that the model will have a harder time finding the anomaly since in most cases QCD jets can be seen as a top jet whose prongs are extremely close together. This means in turn that the reconstruction performance of our model will be worse when the QCD jets are the anomaly.</p> <p>The code can be found on my github repository alongside some commentary, unfortunately the dataset is not available anymore, therefore there is no visible results but the code is still up for reading alongside some commentary. The requirements of the sheet can be downloaded below.</p> <p>Download sheet requirements</p>"},{"location":"bachelor/readme/","title":"Bachelor Thesis","text":"<p>The project can be found on Github (Bachelor project)</p>  Introduction  <p>My bachelor project consisted in the development of a microservice-based web application to securely handle sensitive user data for a company. The project was presented in July 2021.</p> <p>The project consisted of six components:</p> <ul> <li>Frontend - Developed using React.js</li> </ul> <p>The following components developed using Spring Boot:</p> <ul> <li> <p>Gateway - The entry point of the application</p> </li> <li> <p>Security microservice</p> </li> <li> <p>Backend microservice</p> </li> </ul> <p>And then two MySQL databases:</p> <ul> <li> <p>Security database - which handles only the basic user information (email, password, isEnabled)</p> </li> <li> <p>Primary database - which handles the rest of the user information</p> </li> </ul> <p>To handle the distribution of the microservices both Dockerfile and Docker-compose were used. The UI of the project is shown in the following figure.</p> Home of the user view  Functionalities  <p>The application allows the generic user to:</p> <ul> <li> <p>Register</p> </li> <li> <p>Login</p> </li> <li> <p>Update their information (both personal and linked to hard and soft skills that they might have)</p> </li> </ul> <p>The top level user is able to:</p> <ul> <li> <p>Cancel users</p> </li> <li> <p>Search any user inside the database and view or modify their information</p> </li> <li> <p>Disable users</p> </li> </ul> <p>The docker-compose.yaml file was written as follows, since the application was only for demo pourposes the database passwords were as easy as possible and no information was encrypted.</p> <pre><code>version: \"3.9\"\n\nservices:\n  userdb:\n    container_name: user_database\n    image: mysql\n    restart: always\n    environment:\n      - MYSQL_ROOT_PASSWORD=password\n      - MYSQL_DATABASE=database-utenti\n    ports:\n      - \"3307:3306\"\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-u\", \"root\", \"-p password\"]\n      timeout: 20s\n      retries: 15\n\n  securitydb:\n    container_name: security_database\n    image: mysql\n    restart: always\n    environment:\n      - MYSQL_ROOT_PASSWORD=password\n      - MYSQL_DATABASE=database-sicurezza\n    ports:\n      - \"3308:3306\"\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-u\", \"root\", \"-p password\"]\n      timeout: 15s\n      retries: 15\n\n  blue-whale:\n    container_name: gatewayB\n    build:\n      context: ./backend-gateway\n      dockerfile: \"Dockerfile\"\n    ports:\n      - \"8080:8080\"\n\n  sleepin-cat:\n    container_name: usersB\n    build:\n      context: ./backend-gestione-utenti\n      dockerfile: \"Dockerfile\"\n    depends_on:\n      userdb:\n        condition: service_healthy\n    ports:\n      - \"8082:8082\"\n\n  honeybear:\n    container_name: securityB\n    build:\n      context: ./backend-login\n      dockerfile: \"Dockerfile\"\n    depends_on:\n      securitydb:\n        condition: service_healthy\n    ports:\n      - \"8081:8081\"\n\n  frontend:\n    container_name: frontend\n    build:\n      context: ./frontend\n      dockerfile: \"Dockerfile\"\n    ports:\n      - \"3000:3000\"\n</code></pre> <p>The final thesis for the project can be found on Github as well as the presentation I put together for the final bachelor thesis discussion. The download links are the following:</p> <p>Download last version of the thesis</p> <p>Download last version of the presentation</p>"},{"location":"deployment_of_llms_in_6G_networks/readme/","title":"Deployment of LLMs at the edge of the 6G network","text":"<p>This project was a research paper written for the course of Cloud Computing Technologies held by Professors Claudio Ardagna and Marco Anisetti at Universit\u00e0 degli studi di Milano.</p> <p>The idea of the project came from me and was further expanded by me because I was captured by something I read a while back which the following question was asked: \"What if 5G was enough? Do we really need 6G?\".</p> <p>My research is meant to do some paper-reviewing as well as try to explain the areas of major interest in the field of distributed learning techniques (which I identify as the only solutions currently available to handle in a timely fashion models as big as LLMs).</p> <p>In the paper I go through the following steps:</p> <ul> <li> <p>Introduction.</p> </li> <li> <p>6G overview: using a whitepaper from Nokia, I go through the 6 key aspect of the new 6G communication standard putting more emphasis on the fact that 6G should be a network that is shaped to the user's need and liking and therefore it's going to make heavy use of AI approaches like LLMs.</p> </li> <li> <p>Opportunities: I use this section to briefly expose opportunities that have been identified in the literature and then I move to explaining my idea of the architecture, that builds upon the spectrum distribution showed in the previous section.</p> </li> <li> <p>Technical limitations: I use this section to expose some of the evident problems connected to the need of having LLMs stored in the network in locations that are as close as possible to the end users and I also propose some solutions found in literature in the field of distributed learning (Federated Learning, Split Learning and LoRA).</p> </li> </ul> <p>The pdf for both the paper and the presentation can be found at the links below</p> <p>Download the paper</p> <p>Download the presentation</p>"},{"location":"greenfield/readme/","title":"Greenfield","text":"Greenfield <p>The project can be found on Github (Greenfield project)</p> Introduction <p>In this repository is contained all of the code for the project of Distributed and Pervasive Systems by Professor Ardagna and Professor Bettini at Universit\u00e0 degli Studi di Milano, the project was presented in September 2023. In the README file is contained the analysis of the work I did as well as some informations regarding things that I would like to do in the future to make it better. Before beginning the explanation I would like to point out that throughout this file I will generally talk about robots and processes really freely, they are the same thing, threads are of course light-processes that are sons of the <code>Main</code> class that is associated to any Robot. In the project we were required to simulate a smart city's cleaning system, we had a set of processes roaming around and collecting pollution data that were autonomously sent to the server, each of the robots had a 10 percent probability of having problems and needing to go to a mechanic for repairs. The mechanic was able to handle one repair process at a time thus the project needed to handle different requests arriving at the mechanic in parallel. The full project description can be found in ./project_assignement.</p> Structure of the solution <p>The following is the list of all the classes that I chose to implement for this project</p> <ul> <li>Main</li> <li>Admin Client</li> <li>Admin Server<ul> <li>Server Rest Interface</li> <li>MQTT Subscriber</li> <li>Bot Positions</li> <li>Average List</li> </ul> </li> <li>Cleaning Bot<ul> <li>Bot Services</li> <li>Service Comparator</li> <li>Waiting Thread</li> <li>Bot Thread</li> <li>Eliminator Thread</li> <li>Fix Helper Thread</li> <li>GRPC Services Thread (GRPC server)</li> <li>Input Thread</li> <li>Maintenance Thread</li> <li>Measurement Gathering Thread</li> <li>Mutual Exclusion Thread</li> <li>Pollution Sensor Thread</li> <li>Quit Helper Thread</li> <li>Bot Identity Comparator</li> <li>Bot Utilities</li> <li>Comm Pair</li> <li>Measurement Buffer</li> <li>Position</li> </ul> </li> <li>Extra<ul> <li>Atomic Counter</li> <li>Atomic Flag</li> <li>Logger</li> <li>Thread Safe data structure wrapper</li> </ul> </li> <li>Simulator Code</li> <li>Variables</li> </ul> <p>My implementation is quite thread heavy, but I wanted to really experiment as much as possible with it, to get to know and understand how threads work and how to control them while accessing information together.</p> Operations <p>In the following I will discuss my implementation for the different operations.</p> Join <p>When a new process is created it starts the initiator thread, which is simply a thread that instanciates and starts all of the necessary services (e.g. the GRPC Services Thread). in the meanwhile it requests the server to join the network, the join operation currently consists of two parts:</p> <ul> <li>Addition of the new robot to the Admin Server's local structure</li> <li>Communication of personal data (i.e. an instance of the BotIdentity class) to all of the robots in the network</li> </ul> <p>The GRPC communication is asynchronous, and I decided to do two different HTTP requests (check ID availability, communicate my existence to the network) to highlight the difference between the two tasks. The position in the city is chosen by the Admin Server simply by checking in the system which one is the district with the lowest current density. Originally it was supposed to pick a random district uniformly, while the solution would tend to a stable distribution (if enough processes were initialized), it would not be stable for the majority of the low density cases.</p> Comm Pair <p>The <code>CommPair</code> class is a simple toy I came up with, it's nothing more than a pair containing a <code>ManagedChannel</code> and a <code>GRPCStub</code>. The idea behind it is simply to keep on recycling open stubs and channels; thus saving the resources necessary to open a new one each and every time a communication has to take place. Since the project is required to rely heavily on GRPC communication. Comm Pairs are clustered into <code>ThreadSafeHashMaps</code>, each of the pairs is associated to the hash of the <code>BotIdentity</code> the <code>ManagedChannel</code> connects to.</p> Maintenance process <p>The maintenance process i s handled by two different threads, one deals with checking whether the robot should go under maintenance or not, the other built and initialized when the robot undergoes maintenance and handles communication and the maintenance procedure. The maintenance process uses a new thread to carry out the function, something that I didn't mention above was that, to handle potential concurrent removals and insertions, any operation involving the network to its full extent would be done by taking a snapshot of the network. The GRPC is asynchronous and carried out in parallel, any removals necessary are postponed until the time for maintenance is up, thus the removal and stabilization (which do not come cheap) are done at most once per process.</p> Pollution measurements <p>Pollution measurements (more about this later) stop during maintenance, originally they would still be sent, but I decided against it becasue it would not make much sense.</p> Removal <p>The solution I implemented to handle removals is overengineered and overkill for the job and the setup of the project. The reason behind it is that it was originally thought to straighten even the worst distributions (e.g. 1-6, 2-0, 3-0, 4-0), keep in mind that I originally placed robots in the city randomly. The elimination works this way:</p> <ul> <li>I remove mentions of the dead robot from the local machine</li> <li>I contact the Admin Server to let him know that there are dead robots in the city</li> <li>Contact the other robots in the network to remove the references to dead robots from their systems</li> <li>Stabilize the data structure</li> </ul> <p>It's important to mention a couple of details</p> GRPC calls <p>For the elimination procedure, two different GRPC procedures have been called:</p> <ul> <li><code>moveRequest</code> -&gt; which is used by the robot dealing with the elimination process to tell another robot to move away from a district</li> <li><code>positionModificationRequest</code> -&gt; which is more like a notification, when a robot moves to another district computes a random position inside that district and lets every robot in the network know that its position is changed.</li> </ul> <p>Whenever a robot receives a <code>moveRequest</code> the thread waits until the position is changed and the robots in the network have been notified to perform the <code>onComplete</code> and <code>onNext</code> operations.</p> How robots are chosen <p>During the elimination procedure it's important to make sure that everyone is on the same page, to be sure about that I created a setup phase before stabilization where an auxiliary data structure is built. I decided to use a list of Priority Queues with a specific Comparator that allows me to do sorting on the robots by ID. The stabilization is later carried out just by moving around the top of the queues.</p> Pollution sensor <p>The pollution sensor is composed by four different entities:</p> <ul> <li>Simulator</li> <li>Measurement Buffer</li> <li>Measurement Gathering Thread</li> <li>Pollution Sensor Thread</li> </ul> <p>The Measurement Buffer implements a read/write cycle depending on the available size of the buffer. The cycle works unless the robot is on maintenance. The Measurement Gathering Thread keeps averages in memory until the Pollution Sensor Thread comes and takes them from him. This Thread, the same way the other did, stays on hold until the maintenance process is done. The Pollution Sensor Thread is the timer of the pipeline and handles MQTT broadcasting. Each MQTT broadcast will be at most 25 seconds apart from the one before, that is because I wanted to simulate the fact that the robot being broken down. To conclude the Admin Server will receive an MQTT message every: $15$ seconds + $min($ <code>REMAINING_MAINTENANCE_TIME</code> $, 10)$</p> FIX and QUIT commands <p>Both FIX and QUIT commands are being handled through dedicated threads, created once the commands are typed. To be fair the existence of a specialized thread just to send a process to the mechanic is really useless and could be avoided altogether. The QUIT command is a bit more complicated and having a thread in that case is a bit more useful, because if it has to wait doesn't lock the entirety of the input pipeline, but could still be removed in favour of a synchronized function.</p>"},{"location":"structures/readme/","title":"Discrete structures 1","text":"<p>The project can be found on GitHub (Discrete Structures 1 project)</p>  Introduction  <p>The idea behind this project was to provide students with an open source version of the notes for the course of Discrete Structures 1 held by professor Bertille and professor Joos at university Karl Ruprecht of Heidelberg, the notes are in english and have not been updated since the beginning of this year.</p> <p>These notes are currently undergoing some maintenance, therefore the last pdf version is not in the best shape, a future release is bound to fix its appearance. The latest version of the notes can be downloaded at the link below.</p> <p>Download DS1 notes</p>"},{"location":"vdp/readme/","title":"Pump down the flame","text":"<p>The original project page with the download links can be found here Polimi Game Collective.</p> <p>Pump down the flame was a project I developed alongside Federico Maglione, Fabio Patella and Nicol\u00f2 Fasulo to participate in the 2022 Videogame Design and Development course. The game was a 2d level-based platformer in which the objective was to save a series of hostages before the time ran out and without dying.</p> <p>The game was developed using Unity and the sprites were a mix of free asstes and assets created using midjourney and stable diffusion. The game was developed in a span of circa 3 months and was one of the most appreciated games of the course gaining us an overall 6th place out of 24 games and a final grade of 27/30.</p>"}]}